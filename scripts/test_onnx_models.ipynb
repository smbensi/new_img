{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "\"\"\"ONNX Runtime is a performance-focused engine for ONNX models, which \n",
    "inferences efficiently across multiple platforms and hardware (Windows, \n",
    "Linux, and Mac and on both CPUs and GPUs).\"\"\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "model_wo_params = onnxruntime.InferenceSession(\"/home/nvidia/dev/img_new/resnet_v1.onnx\", \n",
    "                                               providers=['CPUExecutionProvider', 'AzureExecutionProvider'])\n",
    "model_with_params = onnxruntime.InferenceSession(\"/home/nvidia/dev/img_new/resnet_v1_with_params.onnx\",\n",
    "                                                 providers=['CPUExecutionProvider', 'AzureExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.ones((1,3,160,160),dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "output_wo_params = np.array(model_wo_params.run(None, {\"input\": img})[0][0])\n",
    "output_with_params = np.array(model_with_params.run(None, {\"input\": img})[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(output_wo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_orig_pt = np.load(\"/home/nvidia/dev/img_new/scripts/ones_orig.npy\")\n",
    "ones_triton = np.load('ones_new_triton.npy')\n",
    "out_orig_pt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019887049"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_pt = out_orig_pt[0]\n",
    "np.linalg.norm(ones_triton - output_wo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'onnx.onnx_ml_pb2.ModelProto'>\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.checker\n",
    "\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"/home/nvidia/dev/img_new/resnet_v1_with_params.onnx\")\n",
    "\n",
    "# Check the model for errors\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "print(type(model))\n",
    "\n",
    "# # Print the model's input and output nodes\n",
    "# print(checker.get_input_names())\n",
    "# print(checker.get_output_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'onnx.helper' has no attribute 'print_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m/home/nvidia/dev/img_new/resnet_v1_with_params.onnx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Print the model's graph\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[39mprint\u001b[39m(onnx\u001b[39m.\u001b[39;49mhelper\u001b[39m.\u001b[39;49mprint_model(model))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'onnx.helper' has no attribute 'print_model'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnx.helper\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"/home/nvidia/dev/img_new/resnet_v1_with_params.onnx\")\n",
    "\n",
    "# Print the model's graph\n",
    "print(onnx.helper.print_model(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onnx.onnx_ml_pb2.ModelProto"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "\"\"\"\n",
    "onnx.onnx_ml_pb2.ModelProto is a Python class that represents an \n",
    "ONNX model in Protocol Buffer (protobuf) format.\n",
    "\n",
    "When you load an ONNX model using the onnx.load() function, it returns an \n",
    "instance of the ModelProto class, which contains the model's graph, \n",
    "inputs, outputs, and other properties.\n",
    "\"\"\"\n",
    "model = onnx.load(\"/home/nvidia/dev/img_new/resnet_v1_with_params.onnx\")\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Node: input\n",
      "  Type: tensor_type {\n",
      "  elem_type: 1\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"input_dynamic_axes_1\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 3\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 160\n",
      "    }\n",
      "    dim {\n",
      "      dim_value: 160\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "  Shape: [dim_param: \"input_dynamic_axes_1\"\n",
      ", dim_value: 3\n",
      ", dim_value: 160\n",
      ", dim_value: 160\n",
      "]\n",
      "Output Node: embedding\n",
      "  Type: tensor_type {\n",
      "  elem_type: 1\n",
      "  shape {\n",
      "    dim {\n",
      "      dim_param: \"embedding_dynamic_axes_1\"\n",
      "    }\n",
      "    dim {\n",
      "      dim_param: \"Divembedding_dim_1\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "  Shape: [dim_param: \"embedding_dynamic_axes_1\"\n",
      ", dim_param: \"Divembedding_dim_1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Get the input and output nodes\n",
    "input_nodes = model.graph.input\n",
    "output_nodes = model.graph.output\n",
    "\n",
    "# Print the input and output nodes\n",
    "for node in input_nodes:\n",
    "    print(f\"Input Node: {node.name}\")\n",
    "    print(f\"  Type: {node.type}\")\n",
    "    print(f\"  Shape: {node.type.tensor_type.shape.dim}\")\n",
    "\n",
    "for node in output_nodes:\n",
    "    print(f\"Output Node: {node.name}\")\n",
    "    print(f\"  Type: {node.type}\")\n",
    "    print(f\"  Shape: {node.type.tensor_type.shape.dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_url = 'http://localhost:8000/resnet'\n",
    "from img_xtend.utils.triton import TritonRemoteModel \n",
    "model = TritonRemoteModel(triton_url)\n",
    "\n",
    "img = np.ones((3,160,160),dtype=np.float32)\n",
    "output_triton = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019887049"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(output_wo_params - output_triton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "onnx_path = '/home/nvidia/dev/img_new/resnet_v1_with_params.onnx'\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(onnx_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "dummy_input = np.random.rand(1, 3, 160, 160).astype(np.float32)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "# ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(dummy_input)}\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: dummy_input}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "output_triton = model(dummy_input[0])\n",
    "print(len(output_triton))\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(output_triton, ort_outs[0],rtol=1e-4, atol=1e-03)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13819055 0.40305853 0.3603325  0.04665537 0.7711553 ]\n",
      " [0.35251063 0.42294842 0.61401653 0.5975801  0.20294605]\n",
      " [0.5354726  0.20468417 0.94154245 0.1505218  0.10396036]\n",
      " [0.34416866 0.9045293  0.5529458  0.8818277  0.14427853]\n",
      " [0.8166203  0.90845484 0.47745967 0.49778247 0.45150957]]\n"
     ]
    }
   ],
   "source": [
    "dummy_input = np.random.rand(1, 3, 160, 160).astype(np.float32)\n",
    "print(dummy_input[0,0,:5,:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
